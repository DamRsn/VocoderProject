{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch corrector and vocoder algorithm presentation\n",
    "This notebook shows the different algorithms used in this project by pich correcting and hamornizing Freddy Mercury's voice !\n",
    "\n",
    "Note that the algorithms used in the final version of the audio plugin (implemented in C++ with JUCE) may sligtly differ from what is done in this python notebook. This code was used only for proof of concept. In C++, optimizations have been added and certain paraters as the frame sizes may be different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sp\n",
    "import wavio\n",
    "import numpy as np\n",
    "import IPython\n",
    "from methods import *\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here is the main sound sample with which we will work with in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wave file\n",
    "name = \"SoundSamples/queen_F\"\n",
    "wavefile_name = name + '.wav'\n",
    "wav_obj = wavio.read(wavefile_name)\n",
    "x_1 = wav_obj.data[:, 0]\n",
    "# get double between -1 and 1\n",
    "x_1 = np.array(x_1, dtype=np.float64)/32767.0\n",
    "\n",
    "# Sampling frequency of audio signal in Hz\n",
    "f_s = wav_obj.rate\n",
    "IPython.display.Audio(x_1, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Corrector\n",
    "Let's first define all the parameters. The sound sample is in F major (one note in the sample is out of key ...). The sampling rate is 44100 Hz and a window of 1500 samples is used. Two consecutives windows overlap on 400 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key\n",
    "key = 'F'\n",
    "notes_freq, notes_str = build_notes_vector(key)\n",
    "\n",
    "# Order of LPC\n",
    "p = 30\n",
    "\n",
    "# Parameters for pitch detector\n",
    "f_min = 90\n",
    "f_max = 600\n",
    "\n",
    "# Pitch marking on valleys or peaks\n",
    "valley = True\n",
    "\n",
    "# Frame and hop size\n",
    "w_len = 1500\n",
    "overlap = 400/w_len\n",
    "hop = int(w_len*(1-overlap))\n",
    "\n",
    "# Delta (search area for pitch marks)\n",
    "delta = 0.94\n",
    "\n",
    "# Tolerance for yin algorithm\n",
    "yin_tol = 0.25\n",
    "\n",
    "# Max lag for pitch detection\n",
    "tau_max = int(np.floor(f_s/f_min))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synthesis window for overlap and add of adjacent frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis window for overlap\n",
    "st_window = np.ones(w_len)\n",
    "\n",
    "# half Hann sides\n",
    "st_window[:int(overlap*w_len)] = hann(int(2*overlap*w_len))[:int(overlap*w_len)]\n",
    "st_window[-int(overlap*w_len):] = hann(int(2*overlap*w_len))[-int(overlap*w_len):]\n",
    "\n",
    "plt.plot(st_window)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.title(\"Synthesis window for frame\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the pitch correction function\n",
    "All the other function used are implemented in the methods.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_corrector(x, w_len, hop, st_window, key, delta, yin_tol, valley, f_s, f_min, f_max):\n",
    "    # Arrays inits\n",
    "    out = np.zeros_like(x, dtype=np.float64)\n",
    "    yin_temp = np.zeros(tau_max, dtype=np.float64)\n",
    "    pitch_arr = np.zeros((x.shape[0]-w_len)//hop+1, dtype=np.float64)\n",
    "    out_window = np.zeros(w_len, dtype=np.float64)\n",
    "\n",
    "\n",
    "    # Previous value inits\n",
    "    prev_pitch = 5.0\n",
    "    prev_voiced_pitch = 5.0\n",
    "    prev_an_marks = np.array([])\n",
    "    prev_st_marks = np.array([])\n",
    "\n",
    "    k = 0\n",
    "    # Start of the processing\n",
    "    for i in range(tau_max, x.shape[0] - w_len, hop):\n",
    "        # Get frame of current index\n",
    "        x_frame = x[i: i+w_len]\n",
    "\n",
    "        # Get pitch\n",
    "        pitch = yin_algo(x, i, yin_temp, w_len, f_s, f_min, f_max, yin_tol)\n",
    "        pitch_arr[k] = pitch\n",
    "\n",
    "        # Get closest note, compute beta ...\n",
    "\n",
    "        # if the current frame is voiced\n",
    "        if pitch > 10:\n",
    "            note_idx = np.argmin(np.abs(pitch-notes_freq))\n",
    "            target_freq = notes_freq[note_idx]\n",
    "            target_note_str = notes_str[note_idx]\n",
    "            beta = target_freq/pitch\n",
    "\n",
    "        # if a previous frame has been voiced\n",
    "        elif prev_voiced_pitch > 10:\n",
    "            note_idx = np.argmin(np.abs(prev_voiced_pitch - notes_freq))\n",
    "            target_freq = notes_freq[note_idx]\n",
    "            target_note_str = notes_str[note_idx]\n",
    "            beta = target_freq / prev_voiced_pitch\n",
    "\n",
    "        # if its the beginning of the processing\n",
    "        else:\n",
    "            beta = 1\n",
    "\n",
    "        # Get analysis pitch marks\n",
    "        if k > 0:\n",
    "            prev_an_marks = np.copy(an_marks)\n",
    "            prev_pitch = pitch_arr[k-1]\n",
    "            if prev_pitch > 10:\n",
    "                prev_voiced_pitch = prev_pitch\n",
    "\n",
    "        an_marks = pitch_marks(x_frame, pitch, prev_an_marks, prev_pitch, prev_voiced_pitch,\n",
    "                                    w_len, hop, f_s, delta, valley)\n",
    "\n",
    "        # Place synthesis pitch marks\n",
    "        st_marks = synthesis_pitch_marks(pitch, prev_pitch, prev_voiced_pitch, an_marks, prev_st_marks, beta,\n",
    "                         w_len, hop, f_s)\n",
    "\n",
    "        # LPC, get coefficients and then prediction error\n",
    "        a = lpc(x_frame, p)\n",
    "\n",
    "        # Filter also some samples outside the window, because can be needed \n",
    "        e = sp.lfilter(a, [1], x[i - tau_max: i + w_len])\n",
    "\n",
    "        # Clear out_window\n",
    "        out_window = out_window * 0.0\n",
    "\n",
    "        # Psola algorithm on residual e\n",
    "        out_window = pitch_shift(e, out_window, pitch, prev_voiced_pitch, an_marks, st_marks, beta, w_len, f_s, \n",
    "                                 tau_max)\n",
    "\n",
    "        prev_st_marks = st_marks\n",
    "\n",
    "        # IIR filtering to get back real sound and add windowed frame to full output array\n",
    "        out[i: i+w_len] = out[i: i+w_len] + sp.lfilter([1], a, out_window) * st_window\n",
    "\n",
    "        k = k + 1\n",
    "        \n",
    "    return out, pitch_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction of an in tune sample\n",
    "Let's pitch correct Freddy Mercury's voice (even if he's obviously singing in tune !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1, pitch_array = pitch_corrector(x_1, w_len, hop, st_window, key, delta, yin_tol, valley, f_s, f_min, f_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear how it sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(out_1, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pointed in the report, this pitch shifting algorithm corrects the voice too quickly ! This makes the transition between notes too abrupt and cancel the voice vibrato. But thanks to this, Freddy could easily be in today's music chart ! \n",
    "\n",
    "#### Correction of an out of tune sample\n",
    "In order to see if the pitch correction really works, let's try it on a voice that really needs pitch correction ! As I quite like this \"Don't stop me now\" sound sample, I detuned it to make it artificially sounds bad. \n",
    "\n",
    "Here is how it sounds ! (sorry about that ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wave file\n",
    "name = \"SoundSamples/queen_F_detuned\"\n",
    "wavefile_name = name + '.wav'\n",
    "wav_obj = wavio.read(wavefile_name)\n",
    "x_detuned = wav_obj.data[:, 0]\n",
    "# get double between -1 and 1\n",
    "x_detuned = np.array(x_detuned, dtype=np.float64)/32767.0\n",
    "\n",
    "# Sampling frequency of audio signal in Hz\n",
    "f_s = wav_obj.rate\n",
    "\n",
    "IPython.display.Audio(x_detuned, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's urgently correct this !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_detuned, pitch_array = pitch_corrector(x_detuned, w_len, hop, st_window, key, delta, yin_tol, valley, f_s, f_min, f_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the corrected sample sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(out_detuned, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voice is sucessfully corrected ! But the correction is quite unstable. The target note often oscillates between two notes in the key. This implementation is too sensible to vibrato in the voice and struggles to find the correct closest note in a stable way ! A fix for this could be to implement a tool that allows to choose the target pitch in post production !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocoder\n",
    "Let's now focus on the vocoder. We will still use that Queen sound sample but with an additional sound where chords are played on a synthesizer with sawtooth like waves. Here's how sounds the synthesizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wave file\n",
    "wavefile_name = \"SoundSamples/dont_stop_me_now_voice.wav\"\n",
    "wav_obj = wavio.read(wavefile_name)\n",
    "\n",
    "# Sampling frequency of audio signal in Hz\n",
    "f_s = wav_obj.rate\n",
    "\n",
    "x = wav_obj.data[:, 0]\n",
    "wavefile_name_synth = \"SoundSamples/dont_stop_me_now_synth.wav\"\n",
    "wav_obj_synth = wavio.read(wavefile_name_synth)\n",
    "y = wav_obj_synth.data[:, 0]\n",
    "\n",
    "x = np.array(x).astype(np.float64)/32767.0\n",
    "y = np.array(y).astype(np.float64)/32767.0\n",
    "\n",
    "IPython.display.Audio(y, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the cross synthesis using the LPC technique with the voice sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some parameters\n",
    "# Size and hop size\n",
    "overlap = 0.75\n",
    "window_len = 512\n",
    "hop = int(window_len*(1-overlap))\n",
    "window_type = 'sine'\n",
    "\n",
    "# Analysis and Synthesis sine window for overlap\n",
    "window = create_window(window_len, overlap, window_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the vocode function \n",
    "\n",
    "def vocode(x, y, window, window_len, hop, order_lpc):\n",
    "    out = np.zeros_like(x).astype(np.float64)\n",
    "    k = 0\n",
    "    p = order_lpc\n",
    "\n",
    "    for i in range(0, x.shape[0] - window_len, hop):\n",
    "        x_frame = x[i: i+window_len] * window\n",
    "        y_frame = y[i: i+window_len] * window\n",
    "\n",
    "        if not silence(x_frame, SILENCE_THRESHOLD):\n",
    "            a = lpc(x_frame, p)\n",
    "            e = sp.lfilter(a, [1], x_frame)\n",
    "\n",
    "            mean_e_voice = np.mean(e**2)\n",
    "            mean_synth = np.mean(y_frame**2)\n",
    "\n",
    "            if np.sqrt(mean_synth) > 1e-4:\n",
    "                g = np.sqrt(mean_e_voice/mean_synth)\n",
    "            else:\n",
    "                g = 0\n",
    "\n",
    "            out[i: i+window_len] = out[i: i+window_len] + sp.lfilter([1], a, g * y_frame) * window\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear the result with a low LPC order of 10 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vocode(x, y, window, window_len, hop, order_lpc=10)\n",
    "IPython.display.Audio(out, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synth sound is well modulated and filtered by the voice but it sounds a bit too robotic !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a larger order of 100 to see the difference. Processing can take some time (< 1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vocode(x, y, window, window_len, hop, order_lpc=100)\n",
    "IPython.display.Audio(out, rate = f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high order of the filter gives a very precise frequency response. We're now able to hear clearly the voice and understand the words. The output is smoother an ressemble less to a synth sound than with order 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish, let's now mix the original voice with the vocoded signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(out + x[:len(x)], rate = f_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
